{"name":"Prediction-of-the-excercise-pattern","tagline":"","body":"Prediction of the Excercise Manner\r\n========================================================\r\n\r\n## Loading liabraries and data\r\n\r\n```{r loading, message=FALSE}\r\nrequire(gbm)\r\n\r\n# download.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\",\r\n#              destfile=\"~/training.csv\",method='wget')\r\ntraining = read.csv(\"~/training.csv\")\r\n# download.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\",\r\n#               destfile=\"~/testing.csv\",method='wget')\r\ntesting = read.csv(\"~/testing.csv\")\r\n```\r\n\r\n## Explore training dataset\r\n\r\n* observation vs. variables ... as is\r\n* observation vs. variables without NAs\r\n* number of columns which contains NA\r\n* all those columns contain the same amount of NAs 19216 which is almost 98% of all observations\r\n * there is no racional reason to impute the data\r\n\r\n```{r exploration}\r\ndim(training)\r\ndim(na.omit(training))\r\ncolSumsNA = colSums(is.na(training))\r\nsum(colSumsNA > 0)\r\nall(colSumsNA[colSumsNA>0] == 19216)\r\n```\r\n\r\n## Clean training dataset\r\n\r\n* __remove variables:__ 'X', 'cvtd_timestamp', all which conatin NA and empty observation record\r\n\r\n```{r clean training}\r\ntraining = training[, -c(1, 5,\r\n                         which(colSums(is.na(training)) > 0),\r\n                         which(colSums(training == \"\") > 0))]\r\n```\r\n\r\n## Transform training dataset variables\r\n\r\n* transform factor varibales into binary variable\r\n\r\n```{r transform training}\r\nfactCols = c(1,4)\r\nfor(colt in factCols) {    \r\n    d = as.data.frame(training[,colt])\r\n    for(i in levels(d[,1])) {\r\n        d = cbind(d, (i == d[,1]) * 1)\r\n        colnames(d)[ncol(d)] = paste0(colnames(training)[colt],\"_\",i)\r\n    }\r\n    training = cbind(training, d[,-1])\r\n}\r\ntraining = training[,-c(factCols)]\r\nd = training[,57:64]\r\nd[d == 0]=-1\r\ntraining[,57:64] = d\r\n```\r\n\r\n## Create model\r\n\r\n* I choosed Gradient Boosting Method for multinomial distribution\r\n * it provides very high accuracy but interpretability is bad\r\n * this kind of method is based on boosting trees\r\n* shown configuration is underfitted - see the picture\r\n * I recommend increase n.tree parameter - it improves accuracy\r\n * I recommend increase cv.fold parameter - it improves robust of the model\r\n\r\n### Expectation\r\n\r\n* I used only 4 fold cross validation because of computational difficulty. You can imporove robust of the model set the cross validation about 10 fold. Robust model means to have a model probably not to optimistic with the training data but we can obtain model with a lower error on testing dataset, which it is crucial.\r\n* I chose smaller k - I got smaller vaiance but bigger bias.\r\n```{r modeling, eval=FALSE}\r\nset.seed(123)\r\n\r\nfitMod <- gbm(classe ~ .,\r\n           data = training,\r\n           distribution = \"multinomial\",\r\n           n.tree = 2000,\r\n           shrinkage = 0.001,\r\n           cv.folds = 4,\r\n           bag.fraction = 0.8,\r\n           interaction.depth = 3,\r\n           verbose = FALSE)\r\ngbm.perf( fitMod, method=\"cv\" )\r\n```\r\n\r\n![Multinomial Deviance](https://github.com/lhsb/Prediction-of-the-Excercise-Pattern/blob/master/figure/multinomial_deviance.png)\r\n\r\n## Prepare testing data set\r\n\r\n* __remove variables__: 'X', 'cvtd_timestamp', 'problem_id', all clumns with NAs\r\n* transform factorial variables to the binary variables\r\n* all variables in the test set must have the same format as in training dataset\r\n\r\n```{r}\r\ntesting = testing[,-c(which(colnames(testing) == \"problem_id\"))]\r\ntesting = testing[, -c(1, 5, which(colSums(is.na(testing)) > 0))]\r\nfactCols = c(1)\r\nfor(colt in factCols) {    \r\n    d = as.data.frame(testing[,colt])\r\n    for(i in levels(d[,1])) {\r\n        d = cbind(d, (i == d[,1]) * 1)\r\n        colnames(d)[ncol(d)] = paste0(colnames(testing)[colt],\"_\",i)\r\n    }\r\n    testing = cbind(testing, d[,-1])\r\n}\r\ntesting = testing[,-c(factCols)]\r\ntesting$new_window_no = 1\r\ntesting$new_window_yes = 0\r\ntesting = testing[,-c(which(colnames(testing) == \"new_window\"))]\r\nd = testing[,56:63]\r\nd[d == 0]=-1\r\ntesting[,56:63] = d\r\n```\r\n\r\n## Predict and prepare output for evaluation\r\n\r\n* there is 80% accuracy on the test data set\r\n* see above how to try to improve this accuracy\r\n\r\n```{r prediction, eval=FALSE}\r\npr = predict(fitMod, testing, type=\"response\")\r\n\r\nanswers = apply(pr, 1, function(x) colnames(pr)[which.max(x)])\r\npml_write_files = function(x){\r\n    n = length(x)\r\n    for(i in 1:n){\r\n        filename = paste0(\"problem_id_\",i,\".txt\")\r\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n    }\r\n}\r\npml_write_files(answers)\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}